{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebebf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import base64\n",
    "import csv\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import altair as alt\n",
    "from altair import expr, datum\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.transforms import Compose\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from itertools import combinations\n",
    "from skimage import io\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b7a42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "Doing a basic EDA we realized that certain images were taken in the same places (they have the same lot/lan) but with different dates. These images, even though they were not identical, were quite similar, so they may bias our model.\n",
    "\n",
    "Problems that can arise with these images:\n",
    "\n",
    "- Data leakage (if we leave the images and proceed without treating them differently, we will have similar images in the training and validation set and the accuracy of our model in our validation set will be higher than it actually is).\n",
    "A solution for this problem is to separate the images by \"groups\" of locations and make sure that each group belongs to either the validation or the test set. This way we are avoiding data leakage and making sure that our model is going to be validated with images that he never encounter before\n",
    "     \n",
    "\n",
    "     \n",
    "- However, when following this approach we may end up overfitting. Since our model is being trained with really similar images. This will cause the model to classify very well the images of the places we have given it but when it encounters new places it will perform badly, i.e. overfitting. To avoid this problem, we must treat our data first. For example, for each group of images, we will detect which ones of them are really similar (using SSIM - Structural Similarity Index) and take out the ones with this index above a certain threshold. By doing this, we will obviously lose some of our data so later on to train our model, we will have to make sure that we create new samples with data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b9ca9-a35e-4d0e-9468-f62c8eb6541b",
   "metadata": {},
   "source": [
    "## Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83884431-e049-48af-a6ed-0864b496d434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network (CNN) model for binary classification.\n",
    "\n",
    "    This CNN model consists of convolutional layers followed by fully connected layers\n",
    "    for binary classification.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (nn.Conv2d): First convolutional layer.\n",
    "        pool1 (nn.MaxPool2d): First max-pooling layer.\n",
    "        conv2 (nn.Conv2d): Second convolutional layer.\n",
    "        pool2 (nn.MaxPool2d): Second max-pooling layer.\n",
    "        conv3 (nn.Conv2d): Third convolutional layer.\n",
    "        pool3 (nn.MaxPool2d): Third max-pooling layer.\n",
    "        fc1 (nn.Linear): First fully connected layer.\n",
    "        fc2 (nn.Linear): Second fully connected layer.\n",
    "        sigmoid (nn.Sigmoid): Sigmoid activation function for binary classification.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self): Initialize the CNN model layers.\n",
    "        forward(self, x): Forward pass through the model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the CNN model layers.\n",
    "\n",
    "        The model includes convolutional layers, max-pooling layers, and fully connected\n",
    "        layers for binary classification.\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # Accepts 1 input channel for grayscale images\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * (img_size[0] // 8) * (img_size[1] // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing binary classification probabilities.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 128 * (img_size[0] // 8) * (img_size[1] // 8))\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c05dca-eeeb-4d41-9c7d-49c2c5b1b225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTIFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch dataset class for working with .tif image files.\n",
    "\n",
    "    This class loads and processes .tif images from a specified directory and provides\n",
    "    them as a PyTorch dataset for training and validation.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory containing .tif image files.\n",
    "        transform (callable, optional): A function/transform to apply to the images.\n",
    "\n",
    "    Attributes:\n",
    "        root (str): The root directory where image files are located.\n",
    "        transform (callable, optional): The transform function for image preprocessing.\n",
    "        file_list (list): List of .tif image file paths.\n",
    "        data (list): List of image data.\n",
    "        labels (list): List of image labels.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, root, transform=None): Initializes the dataset by loading image data.\n",
    "        __len__(self): Returns the number of images in the dataset.\n",
    "        __getitem__(self, idx): Retrieves an image and its label by index.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, root, transform=None):    \n",
    "        \"\"\"\n",
    "        Initialize the CustomTIFDataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory containing .tif image files.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "            \"\"\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "\n",
    "        # Recursively search for .tif files in subdirectories\n",
    "        for dirpath, dirnames, filenames in os.walk(root):\n",
    "            for fname in filenames:\n",
    "                if fname.endswith('.tif'):\n",
    "                    self.file_list.append(os.path.join(dirpath, fname))\n",
    "\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for image_path in self.file_list:\n",
    "            with rasterio.open(image_path) as img:\n",
    "                image = img.read(1).astype(np.float32)  # Adjust the band index as needed\n",
    "\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "\n",
    "                label = 0 if \"no_plume\" in image_path else 1  # Assign labels based on the image path\n",
    "\n",
    "                self.data.append(image)\n",
    "                self.labels.append(label)\n",
    "\n",
    "        # Print the number of images loaded\n",
    "        print(f\"Loaded {len(self.data)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of images in the dataset.\n",
    "            \"\"\"\n",
    "        \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        \"\"\"\n",
    "        Get an image and its label by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image data and its label.\n",
    "            \"\"\"\n",
    "            \n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0253d-5360-4e03-a358-ab8de4d73599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a custom transformation to normalize the tensor\n",
    "class MinMaxNormalize(object):\n",
    "    \"\"\"\n",
    "    A custom transformation to normalize a PyTorch tensor between 0 and 1.\n",
    "\n",
    "    This transformation calculates the minimum and maximum values in the input tensor\n",
    "    and normalizes the tensor so that its values fall within the range [0, 1].\n",
    "\n",
    "    Usage:\n",
    "    transform = MinMaxNormalize()\n",
    "    normalized_tensor = transform(input_tensor)\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Methods:\n",
    "        __call__(self, tensor): Normalize the input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The normalized tensor with values between 0 and 1. \n",
    "        \"\"\"\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Normalize the input tensor.\n",
    "\n",
    "        Args:\n",
    "            tensor (torch.Tensor): The input tensor to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor with values between 0 and 1.\n",
    "            \"\"\"\n",
    "        min_val = tensor.min()\n",
    "        max_val = tensor.max()\n",
    "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "        return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfba0d-b37b-4906-bd87-a6d998c3d817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomRotate(object):\n",
    "    \"\"\"\n",
    "    A custom transformation to rotate an image by a specified angle.\n",
    "\n",
    "    This transformation rotates an image by the specified angle in degrees.\n",
    "\n",
    "    Usage:\n",
    "    transform = CustomRotate(degrees)\n",
    "    rotated_image = transform(input_image)\n",
    "\n",
    "    Args:\n",
    "        degrees (float): The angle in degrees by which to rotate the image.\n",
    "\n",
    "    Methods:\n",
    "        __call__(self, image): Rotate the input image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The rotated image.\n",
    "        \"\"\"\n",
    "        \n",
    "    def __init__(self, degrees):\n",
    "        \"\"\"\n",
    "        Initialize the CustomRotate transformation.\n",
    "\n",
    "        Args:\n",
    "            degrees (float): The angle in degrees by which to rotate the image.\n",
    "            \"\"\"\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        Rotate the input image.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): The input image to be rotated.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The rotated image.\n",
    "            \"\"\"\n",
    "        \n",
    "        image = Image.fromarray(image)\n",
    "        image = image.rotate(self.degrees)\n",
    "        \n",
    "        return np.array(image)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23359b7f-e83e-43ec-b03d-2c423143540c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomHorizontalFlip(object):\n",
    "    \"\"\"\n",
    "    A custom transformation to horizontally flip an image with a given probability.\n",
    "\n",
    "    This transformation horizontally flips an image with the specified probability.\n",
    "\n",
    "    Args:\n",
    "        p (float, optional): The probability of flipping the image (default is 0.5).\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, p): Initialize the CustomHorizontalFlip transformation.\n",
    "        __call__(self, image): Horizontally flip the input image with the specified probability.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The horizontally flipped or original image, depending on the probability.\n",
    "        \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        \"\"\"\n",
    "         Initialize the CustomHorizontalFlip transformation.\n",
    "\n",
    "        Args:\n",
    "            p (float, optional): The probability of flipping the image (default is 0.5).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        \"\"\"\n",
    "        Horizontally flip the input image with the specified probability.\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): The input image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The horizontally flipped or original image, depending on the probability.\n",
    "            \"\"\"\n",
    "        \n",
    "        if np.random.random() < self.p:\n",
    "            return np.fliplr(image).copy()\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a123a-e9d4-4426-8c33-ec5aa3ea561c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate SSIM\n",
    "def calculate_ssim(img1, img2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the Structural Similarity Index (SSIM) between two images.\n",
    "\n",
    "    The SSIM is a measure of structural similarity between two images. It takes into\n",
    "    account luminance, contrast, and structure. A higher SSIM value indicates greater\n",
    "    similarity between the images.\n",
    "\n",
    "    Parameters:\n",
    "    - img1 (numpy.ndarray): The first image (as a NumPy array).\n",
    "    - img2 (numpy.ndarray): The second image (as a NumPy array).\n",
    "\n",
    "    Returns:\n",
    "    - float: The SSIM value, ranging from -1 to 1, with 1 indicating identical images.\n",
    "    \"\"\"\n",
    "    \n",
    "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
    "    window = np.outer(kernel, kernel.transpose())\n",
    "    mu1 = signal.fftconvolve(img1, window, mode='same')\n",
    "    mu2 = signal.fftconvolve(img2, window, mode='same')\n",
    "    mu1_sq = mu1 * mu1\n",
    "    mu2_sq = mu2 * mu2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = signal.fftconvolve(img1 * img1, window, mode='same') - mu1_sq\n",
    "    sigma2_sq = signal.fftconvolve(img2 * img2, window, mode='same') - mu2_sq\n",
    "    sigma12 = signal.fftconvolve(img1 * img2, window, mode='same') - mu1_mu2\n",
    "    ssim_num = (2 * mu1_mu2 + 0.01) * (2 * sigma12 + 0.03)\n",
    "    ssim_den = (mu1_sq + mu2_sq + 0.01) * (sigma1_sq + sigma2_sq + 0.03)\n",
    "    \n",
    "    return np.mean(ssim_num / ssim_den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356a530-8a6e-4513-9560-c39f00b60d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to reverse the min-max normalization\n",
    "def reverse_min_max_normalize(normalized_tensor, min_val, max_val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reverse the Min-Max normalization of a tensor.\n",
    "\n",
    "    This function reverses the Min-Max normalization by scaling the normalized tensor\n",
    "    back to the original range specified by min_val and max_val.\n",
    "\n",
    "    Args:\n",
    "        normalized_tensor (torch.Tensor): The normalized tensor to be reversed.\n",
    "        min_val (float): The minimum value of the original range before normalization.\n",
    "        max_val (float): The maximum value of the original range before normalization.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The reversed tensor with values scaled back to the original range.\n",
    "    \"\"\"   \n",
    "    \n",
    "    reversed_tensor = (normalized_tensor * (max_val - min_val)) + min_val\n",
    "    \n",
    "    return reversed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7b314-b297-4aa3-b14f-7d1d156200dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs, log_interval=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train and evaluate a PyTorch model on a training and validation dataset.\n",
    "\n",
    "    This function trains a model on a training dataset, evaluates its performance on a validation dataset,\n",
    "    and returns the best model based on validation AUC score.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be trained and evaluated.\n",
    "        train_loader (DataLoader): The DataLoader for the training dataset.\n",
    "        valid_loader (DataLoader): The DataLoader for the validation dataset.\n",
    "        criterion (nn.Module): The loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        device (str): The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "        num_epochs (int): The number of training epochs.\n",
    "        log_interval (int, optional): The interval for logging training progress (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The best model based on the highest validation AUC score.\n",
    "        list: List of AUC scores for the training dataset at each epoch.\n",
    "        list: List of AUC scores for the validation dataset at each epoch.\n",
    "        list: List of training loss values at each epoch.\n",
    "        list: List of validation loss values at each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_auc_all = []\n",
    "    valid_auc_all = []\n",
    "    train_loss_all = []\n",
    "    valid_loss_all = []\n",
    "\n",
    "    best_valid_auc = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predicted = outputs\n",
    "            train_predictions.extend(predicted.detach().cpu().numpy().flatten())\n",
    "            train_targets.extend(labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "        train_auc = roc_auc_score(train_targets, train_predictions)\n",
    "        train_loss /= len(train_loader)\n",
    "        train_auc_all.append(train_auc)\n",
    "        train_loss_all.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_predictions = []\n",
    "        valid_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                preds = outputs\n",
    "                valid_predictions.extend(preds.detach().cpu().numpy().flatten())\n",
    "                valid_targets.extend(labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "        valid_auc = roc_auc_score(valid_targets, valid_predictions)\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_auc_all.append(valid_auc)\n",
    "        valid_loss_all.append(valid_loss)\n",
    "\n",
    "        if valid_auc > best_valid_auc:\n",
    "            best_valid_auc = valid_auc\n",
    "            best_model = model\n",
    "\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}, '\n",
    "                  f'Validation Loss: {valid_loss:.4f}, Validation AUC: {valid_auc:.4f}')\n",
    "\n",
    "    return best_model, train_auc_all, valid_auc_all, train_loss_all, valid_loss_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477e73e-0a86-4af5-b3ab-24f8484279e2",
   "metadata": {},
   "source": [
    "## Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ac641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paths to fetch the data\n",
    "path = 'hfactory_magic_folders/cleanr/train data/metadata.csv'\n",
    "path_1 = 'hfactory_magic_folders/cleanr/train data/images/plume'\n",
    "path_2 = 'hfactory_magic_folders/cleanr/train data/images/no_plume'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f3ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Store how many images have the same latitude and longitude (to avoid overfitting)\n",
    "paths = df.groupby([\"lat\",\"lon\",\"plume\"]).count()[['path']].sort_values(\"path\", ascending = False)\n",
    "\n",
    "# Initialize an empty column to store all images path\n",
    "paths['lists'] = np.nan\n",
    "\n",
    "temp = []\n",
    "\n",
    "# Save all images paths that have the same lat and lon in one list\n",
    "for i,row in enumerate(paths.iterrows()):  \n",
    "  row = row[0]\n",
    "  temp.append(list(df['path'][(df['lat'] == row[0]) & (df['lon'] == row[1])]))\n",
    "    \n",
    "paths['lists'] = temp\n",
    "\n",
    "# We don't want anymore the lat, lon and plume to be an index so we are resetting the dataframe\n",
    "paths = paths.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89497d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your into a list\n",
    "\n",
    "directory = \"hfactory_magic_folders/cleanr/train data/\"\n",
    "extension = \".tif\"\n",
    "\n",
    "# Create a list to store the paths of images to keep\n",
    "final_group_plume = []\n",
    "final_group_no_plume = []\n",
    "\n",
    "for i in range(len(paths)):\n",
    "    image_paths = paths.iloc[i][\"lists\"] \n",
    "    plume_or_not = paths.iloc[i][\"plume\"]\n",
    "    image_paths2 = [directory + image_path + extension for image_path in image_paths]\n",
    "    images = [io.imread(path) for path in image_paths2]\n",
    "\n",
    "    # Define the similarity threshold (adjust as needed)\n",
    "    similarity_threshold = 0.7\n",
    "    \n",
    "    # Create a list to store the paths of images to keep of a particular location\n",
    "    final_group_loc = []\n",
    "\n",
    "    # Create a list to track images to keep (initialize with all indices)\n",
    "    images_to_keep = list(range(len(images)))\n",
    "\n",
    "    # Create a set to track images already marked for removal\n",
    "    marked_for_removal = set()\n",
    "\n",
    "    # Calculate SSIM scores and mark duplicates for removal\n",
    "    for i in range(len(images)):\n",
    "        if i not in marked_for_removal:\n",
    "            for j in range(i + 1, len(images)):\n",
    "                if j not in marked_for_removal:\n",
    "                    ssim_score = calculate_ssim(images[i], images[j])\n",
    "                    if ssim_score > similarity_threshold:\n",
    "                        # Mark one of the images for removal (add its index to the list)\n",
    "                        images_to_keep.remove(j)\n",
    "                        marked_for_removal.add(j)\n",
    "\n",
    "    # Add the paths of images to keep to the final_group_loc\n",
    "    final_group_loc.extend([image_paths2[i] for i in images_to_keep])\n",
    "    \n",
    "    # Add the paths of images to keep to the final_group_plume or final_group_not_plume\n",
    "    if plume_or_not == \"yes\":\n",
    "        final_group_plume.extend([final_group_loc])\n",
    "    else:\n",
    "        final_group_no_plume.extend([final_group_loc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056824e6",
   "metadata": {},
   "source": [
    "Now that we have the selected images divided by locations and plume or not plume, lets create our validation and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a4a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the percentage for selection (10%)\n",
    "percentage = 20\n",
    "\n",
    "# Randomly select 10% of locations with plume\n",
    "plume_validation = random.sample(final_group_plume, k=int(0.01 * percentage * len(final_group_plume)))\n",
    "plume_training = [loc for loc in final_group_plume if loc not in plume_validation]\n",
    "\n",
    "# Randomly select 10% of locations with no plume\n",
    "no_plume_validation = random.sample(final_group_no_plume, k=int(0.01 * percentage * len(final_group_no_plume)))\n",
    "no_plume_training = [loc for loc in final_group_no_plume if loc not in no_plume_validation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd747f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths and data_dir\n",
    "val_dir = 'validation'\n",
    "train_dir = 'train'\n",
    "plume_dir = 'plume'\n",
    "no_plume_dir = 'no_plume'\n",
    "\n",
    "# List of subdirectories\n",
    "subdirectories = [plume_dir, no_plume_dir]\n",
    "directories = [val_dir, train_dir]\n",
    "\n",
    "# Clean the validation and train directories\n",
    "for directory in directories:\n",
    "    for subdirectory in subdirectories:\n",
    "        path = os.path.join(directory, subdirectory)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)  # Clean the directory if it already exists\n",
    "        os.makedirs(path, exist_ok=True)  # Create the directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66526b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy files to validation directories\n",
    "for loc in plume_validation:\n",
    "    for filename in loc:\n",
    "        shutil.copy2(filename, \"./validation/plume\")\n",
    "\n",
    "for loc in no_plume_validation:\n",
    "    for filename in loc:\n",
    "        shutil.copy2(filename, \"./validation/no_plume\")\n",
    "\n",
    "for loc in plume_training:\n",
    "    for filename in loc:\n",
    "        shutil.copy2(filename, \"./train/plume\")\n",
    "\n",
    "for loc in no_plume_training:\n",
    "    for filename in loc:\n",
    "        shutil.copy2(filename, \"./train/no_plume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b030bf5",
   "metadata": {},
   "source": [
    "Let's check our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604fa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to your folder containing the images\n",
    "folder_path1 = './train/no_plume'\n",
    "folder_path2 = './train/plume'\n",
    "folder_path3 = './validation/no_plume'\n",
    "folder_path4 = './validation/plume'\n",
    "\n",
    "# Define the file extensions that you consider as images (e.g., jpg, png, etc.)\n",
    "image_extensions = ['tif']  # Add more extensions if needed\n",
    "\n",
    "# Use glob to find all files in the folder with the specified extensions\n",
    "image_files1 = []\n",
    "image_files2 = []\n",
    "image_files3 = []\n",
    "image_files4 = []\n",
    "for ext in image_extensions:\n",
    "    image_files1.extend(glob.glob(os.path.join(folder_path1, f'*.{ext}')))\n",
    "    image_files2.extend(glob.glob(os.path.join(folder_path2, f'*.{ext}')))\n",
    "    image_files3.extend(glob.glob(os.path.join(folder_path3, f'*.{ext}')))\n",
    "    image_files4.extend(glob.glob(os.path.join(folder_path4, f'*.{ext}')))\n",
    "\n",
    "# Count the number of image files\n",
    "num_images1 = len(image_files1)\n",
    "num_images2 = len(image_files2)\n",
    "num_images3 = len(image_files3)\n",
    "num_images4 = len(image_files4)\n",
    "\n",
    "print(f'Total number of images in the folder train no plume: {num_images1}')\n",
    "print(f'Total number of images in the folder train plume: {num_images2}')\n",
    "print(f'Total number of images in the folder validation no plume: {num_images3}')\n",
    "print(f'Total number of images in the folder validation plume: {num_images4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea4b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the paths to the training and validation data\n",
    "train_data_dir = './train'\n",
    "valid_data_dir = './validation'\n",
    "\n",
    "# Set the image size, batch size, and number of classes (1 for binary classification)\n",
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "num_classes = 1  # Binary classification\n",
    "\n",
    "# Define data transformations\n",
    "train_augmentations = Compose([\n",
    "    CustomRotate(degrees=15),\n",
    "    CustomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    MinMaxNormalize(),\n",
    "])\n",
    "\n",
    "train_original_transform = transforms.Compose([\n",
    "    #transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    MinMaxNormalize(),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    #transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    MinMaxNormalize(),\n",
    "])\n",
    "\n",
    "# Create datasets and data loaders\n",
    "# Load original data\n",
    "train_original_dataset = CustomTIFDataset(train_data_dir, transform=train_original_transform)\n",
    "train_augmented_dataset = CustomTIFDataset(train_data_dir, transform=train_augmentations)\n",
    "train_dataset = ConcatDataset([train_original_dataset, train_augmented_dataset])\n",
    "valid_dataset = CustomTIFDataset(valid_data_dir, transform=valid_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0380717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example tensor (image, label)\n",
    "example_tensor, label = valid_dataset[0]\n",
    "\n",
    "# Reverse min-max normalization (assuming min_val and max_val are known)\n",
    "min_val = 0.0\n",
    "max_val = 1.0\n",
    "reversed_tensor = reverse_min_max_normalize(example_tensor, min_val, max_val)\n",
    "\n",
    "# Convert the reversed tensor to a NumPy array\n",
    "reversed_image = reversed_tensor.numpy()\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(reversed_image[0], cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4c091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count \"plume\" and \"no plume\" instances in the training dataset\n",
    "plume_count_train = len([item for item in train_dataset if item[1] == 0])\n",
    "no_plume_count_train = len([item for item in train_dataset if item[1] == 1])\n",
    "\n",
    "# Count \"plume\" and \"no plume\" instances in the validation dataset\n",
    "plume_count_valid = len([item for item in valid_dataset if item[1] == 0])\n",
    "no_plume_count_valid = len([item for item in valid_dataset if item[1] == 1])\n",
    "\n",
    "print(\"Training Data - Plume: {} instances, No Plume: {} instances\".format(plume_count_train, no_plume_count_train))\n",
    "print(\"Validation Data - Plume: {} instances, No Plume: {} instances\".format(plume_count_valid, no_plume_count_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ae009",
   "metadata": {},
   "source": [
    "## Model 1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9d1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ef861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "best_model, train_auc, valid_auc, train_loss, valid_loss = train_and_evaluate_model(\n",
    "    model, train_loader, valid_loader, criterion, optimizer, device, num_epochs, log_interval=1)\n",
    "\n",
    "# Plot the training and validation AUC and loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_auc, label='Train AUC')\n",
    "plt.plot(valid_auc, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation AUC')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99c120",
   "metadata": {},
   "source": [
    "## Model 2: ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceead423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet model (e.g., ResNet-50)\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the first convolutional layer to accept 1 channel grayscale images\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Freeze all layers except the final fully connected layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final fully connected layer to match your problem\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),  # Change the output size for binary classification (1 output neuron)\n",
    "    nn.Sigmoid()  # Use sigmoid activation for binary classification\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters())  # Only optimize the final fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022469e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb529c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "best_model, train_auc, valid_auc, train_loss, valid_loss = train_and_evaluate_model(\n",
    "    model, train_loader, valid_loader, criterion, optimizer, device, num_epochs, log_interval=1)\n",
    "\n",
    "# Plot the training and validation AUC and loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_auc, label='Train AUC')\n",
    "plt.plot(valid_auc, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation AUC')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dfda6",
   "metadata": {},
   "source": [
    "## Model 3: Wide ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d79b4-d042-4746-8bc9-aeefc14ef2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained Wide ResNet-50 model\n",
    "model = models.wide_resnet50_2(pretrained=True)\n",
    "\n",
    "# Modify the first convolutional layer to accept 1 channel grayscale images\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the final fully connected layer to match your problem\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),  # Change the output size for binary classification (1 output neuron)\n",
    "    nn.Sigmoid()  # Use sigmoid activation for binary classification\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104efd7-5c6c-4634-a15f-89fadab195ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70babb51-35da-4f7c-8d50-9b3f71bc9ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "best_model, train_auc, valid_auc, train_loss, valid_loss = train_and_evaluate_model(\n",
    "    model, train_loader, valid_loader, criterion, optimizer, device, num_epochs, log_interval=1)\n",
    "\n",
    "# Plot the training and validation AUC and loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_auc, label='Train AUC')\n",
    "plt.plot(valid_auc, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation AUC')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f76b0",
   "metadata": {},
   "source": [
    "## Model 4: MobilNet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4a6ac-41a4-457f-b362-ed76279022d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained MobileNet model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# Modify the input layer to accept a single channel (grayscale) image\n",
    "model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6767b-2795-4d60-b565-9006fc70d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81721c9-6a47-4b5e-8352-e620c25ded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "best_model, train_auc, valid_auc, train_loss, valid_loss = train_and_evaluate_model(\n",
    "    model, train_loader, valid_loader, criterion, optimizer, device, num_epochs, log_interval=1)\n",
    "\n",
    "# Plot the training and validation AUC and loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_auc, label='Train AUC')\n",
    "plt.plot(valid_auc, label='Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation AUC')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(valid_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
